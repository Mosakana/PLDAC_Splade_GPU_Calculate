{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:40.846123518Z",
     "start_time": "2024-06-23T21:27:39.362558475Z"
    }
   },
   "outputs": [],
   "source": [
    "from optim_relu_max_linear import OptimReluMaxLinear\n",
    "from check_grad_optim_v import test_optim_splade_model\n",
    "import triton\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82f9c9b268a9a9d8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:40.878352517Z",
     "start_time": "2024-06-23T21:27:40.865656448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0418, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "B = 64\n",
    "L = 300\n",
    "D = 200\n",
    "V = 1000\n",
    "\n",
    "x = torch.randn(B, L, D, requires_grad=True, dtype=torch.float64, device='cuda')\n",
    "w = torch.randn(D, V, requires_grad=True, dtype=torch.float64, device='cuda')\n",
    "b = -torch.sqrt(-2 * torch.log(torch.tensor(0.05, device='cuda').repeat(V))) * 10\n",
    "\n",
    "res = x @ w + b.reshape(1, 1, *b.shape)\n",
    "print((res > 0).sum() / (B * L * V))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:41.128451720Z",
     "start_time": "2024-06-23T21:27:40.870627606Z"
    }
   },
   "id": "f8f89bde8573775d",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d80423ce9147901",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:41.128832005Z",
     "start_time": "2024-06-23T21:27:41.125965764Z"
    }
   },
   "outputs": [],
   "source": [
    "UPPER_LIMIT = 11  ## change ca pour diminuer la consommation de memoire.\n",
    "B = 16\n",
    "L = 100\n",
    "D = 200\n",
    "x_vals = [10000 * i for i in range(1, UPPER_LIMIT)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6f0f29253a12a65",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:41.148956872Z",
     "start_time": "2024-06-23T21:27:41.128776902Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10000, 20000, 30000, 40000, 50000, 60000, 70000, 80000, 90000, 100000]\n"
     ]
    }
   ],
   "source": [
    "print(x_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "af71709b893bc49c",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:41.912477872Z",
     "start_time": "2024-06-23T21:27:41.900193006Z"
    }
   },
   "outputs": [],
   "source": [
    "configs = []\n",
    "\n",
    "configs.append(\n",
    "    triton.testing.Benchmark(\n",
    "        x_names=[\"V\"],  # Argument names to use as an x-axis for the plot\n",
    "        x_vals=x_vals,  # Different possible values for `x_name`\n",
    "        line_arg=\"provider\",  # Argument name whose value corresponds to a different line in the plot\n",
    "        # Possible values for `line_arg`\n",
    "        # Don't compare to cublas for fp8 cases as torch.matmul doesn't support fp8 at the moment.\n",
    "        line_vals= [\"torch\", \"triton\"],  # Label name for the lines\n",
    "        line_names= [\"torch\", \"Triton\"],  # Line styles\n",
    "        styles=[(\"green\", \"-\"), (\"blue\", \"-\")],\n",
    "        ylabel=\"GB/s\",  # Label name for the y-axis\n",
    "        plot_name=\"relu_max_backward_performance\",\n",
    "        args={'B' : B, 'L' : L, 'D': D},\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b87be1a52d8318c5",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:42.433844171Z",
     "start_time": "2024-06-23T21:27:42.431783087Z"
    }
   },
   "outputs": [],
   "source": [
    "@triton.testing.perf_report(configs)\n",
    "def benchmark(V, B, L, D, provider):\n",
    "    x = torch.randn(B, L, D, requires_grad=True, dtype=torch.float64, device='cuda')\n",
    "    w = torch.randn(D, V, requires_grad=True, dtype=torch.float64, device='cuda')\n",
    "    b = -torch.sqrt(-2 * torch.log(torch.tensor(0.05, device='cuda').repeat(V))) * 10\n",
    "    list_lengths = torch.randint(0, L, [B])\n",
    "    \n",
    "    mask = torch.ones(B, L).cuda()\n",
    "    for i, l in enumerate(list_lengths):\n",
    "        mask[i, l:] = 0\n",
    "        \n",
    "    mask_inf = torch.where(mask == 1, 0, -torch.inf)\n",
    "    relu = torch.nn.ReLU()\n",
    "    \n",
    "    torch_calculation = relu(torch.max((x @ w) + b.reshape(1, 1, *b.shape) + mask_inf.reshape(*mask_inf.shape, 1), dim=1)[0])\n",
    "    triton_calculation = OptimReluMaxLinear.apply(x, w, b, mask)[0]\n",
    "    \n",
    "    quantiles = [0.5, 0.2, 0.8]\n",
    "    \n",
    "    print(f'---------- V : {V}\\tprovider : {provider} ----------')\n",
    "    \n",
    "    if provider == 'torch':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: torch_calculation.sum().backward(retain_graph=True), quantiles=quantiles)\n",
    "    if provider == 'triton':\n",
    "        ms, min_ms, max_ms = triton.testing.do_bench(lambda: triton_calculation.sum().backward(retain_graph=True) , quantiles=quantiles)\n",
    "    gbps = lambda ms: 16 * (B * L * D + D * V + V) * 1e-9 / (ms * 1e-3)\n",
    "    return gbps(ms), gbps(max_ms), gbps(min_ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "63c3a391bff2fa3f",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-06-23T21:27:50.729620836Z",
     "start_time": "2024-06-23T21:27:43.621744446Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- V : 10000\tprovider : torch ----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m benchmark\u001B[38;5;241m.\u001B[39mrun(show_plots\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, print_data\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n",
      "File \u001B[0;32m~/anaconda3/envs/mlenv/lib/python3.11/site-packages/triton/testing.py:341\u001B[0m, in \u001B[0;36mMark.run\u001B[0;34m(self, show_plots, print_data, save_path, return_df, **kwargs)\u001B[0m\n\u001B[1;32m    339\u001B[0m     html\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<html><body>\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    340\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m bench \u001B[38;5;129;01min\u001B[39;00m benchmarks:\n\u001B[0;32m--> 341\u001B[0m     result_dfs\u001B[38;5;241m.\u001B[39mappend(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run(bench, save_path, show_plots, print_data, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs))\n\u001B[1;32m    342\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m save_path:\n\u001B[1;32m    343\u001B[0m         html\u001B[38;5;241m.\u001B[39mwrite(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m<image src=\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mbench\u001B[38;5;241m.\u001B[39mplot_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.png\u001B[39m\u001B[38;5;130;01m\\\"\u001B[39;00m\u001B[38;5;124m/>\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/mlenv/lib/python3.11/site-packages/triton/testing.py:287\u001B[0m, in \u001B[0;36mMark._run\u001B[0;34m(self, bench, save_path, show_plots, print_data, diff_col, **kwrags)\u001B[0m\n\u001B[1;32m    285\u001B[0m row_mean, row_min, row_max \u001B[38;5;241m=\u001B[39m [], [], []\n\u001B[1;32m    286\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m y \u001B[38;5;129;01min\u001B[39;00m bench\u001B[38;5;241m.\u001B[39mline_vals:\n\u001B[0;32m--> 287\u001B[0m     ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfn(\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mx_args, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m{bench\u001B[38;5;241m.\u001B[39mline_arg: y}, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mbench\u001B[38;5;241m.\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwrags)\n\u001B[1;32m    288\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    289\u001B[0m         y_mean, y_min, y_max \u001B[38;5;241m=\u001B[39m ret\n",
      "Cell \u001B[0;32mIn[7], line 16\u001B[0m, in \u001B[0;36mbenchmark\u001B[0;34m(V, B, L, D, provider)\u001B[0m\n\u001B[1;32m     13\u001B[0m relu \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mnn\u001B[38;5;241m.\u001B[39mReLU()\n\u001B[1;32m     15\u001B[0m torch_calculation \u001B[38;5;241m=\u001B[39m relu(torch\u001B[38;5;241m.\u001B[39mmax((x \u001B[38;5;241m@\u001B[39m w) \u001B[38;5;241m+\u001B[39m b\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m*\u001B[39mb\u001B[38;5;241m.\u001B[39mshape) \u001B[38;5;241m+\u001B[39m mask_inf\u001B[38;5;241m.\u001B[39mreshape(\u001B[38;5;241m*\u001B[39mmask_inf\u001B[38;5;241m.\u001B[39mshape, \u001B[38;5;241m1\u001B[39m), dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)[\u001B[38;5;241m0\u001B[39m])\n\u001B[0;32m---> 16\u001B[0m triton_calculation \u001B[38;5;241m=\u001B[39m OptimReluMaxLinear\u001B[38;5;241m.\u001B[39mapply(x, w, b, mask)[\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m     18\u001B[0m quantiles \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0.5\u001B[39m, \u001B[38;5;241m0.2\u001B[39m, \u001B[38;5;241m0.8\u001B[39m]\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m---------- V : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mV\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\t\u001B[39;00m\u001B[38;5;124mprovider : \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mprovider\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m ----------\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/mlenv/lib/python3.11/site-packages/torch/autograd/function.py:553\u001B[0m, in \u001B[0;36mFunction.apply\u001B[0;34m(cls, *args, **kwargs)\u001B[0m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_C\u001B[38;5;241m.\u001B[39m_are_functorch_transforms_active():\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001B[39;00m\n\u001B[1;32m    552\u001B[0m     args \u001B[38;5;241m=\u001B[39m _functorch\u001B[38;5;241m.\u001B[39mutils\u001B[38;5;241m.\u001B[39munwrap_dead_wrappers(args)\n\u001B[0;32m--> 553\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28msuper\u001B[39m()\u001B[38;5;241m.\u001B[39mapply(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[1;32m    555\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_setup_ctx_defined:\n\u001B[1;32m    556\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mRuntimeError\u001B[39;00m(\n\u001B[1;32m    557\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mIn order to use an autograd.Function with functorch transforms \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    558\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    559\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstaticmethod. For more details, please see \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    560\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    561\u001B[0m     )\n",
      "File \u001B[0;32m~/PLDAC_Splade_GPU_Calculate/src/optim_relu_max_linear/optim_relu_max_linear.py:146\u001B[0m, in \u001B[0;36mOptimReluMaxLinear.forward\u001B[0;34m(x, weight, bias, mask)\u001B[0m\n\u001B[1;32m    143\u001B[0m effective_indice \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m    145\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m b, v \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(\u001B[38;5;241m*\u001B[39mindice_not_zero):\n\u001B[0;32m--> 146\u001B[0m     effective_indice\u001B[38;5;241m.\u001B[39mappend((b, v, max_indice[b, v]))\n\u001B[1;32m    148\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m result, torch\u001B[38;5;241m.\u001B[39mtensor(effective_indice, device\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mcuda\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "benchmark.run(show_plots=True, print_data=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaebe55e449db86",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-10T21:24:08.854779447Z",
     "start_time": "2024-05-10T21:24:08.800116778Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa4757673ecf60af",
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
